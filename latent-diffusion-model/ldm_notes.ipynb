{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e1b051-b465-4a35-8b97-2955b2a0a354",
   "metadata": {},
   "source": [
    "# Conditional Latent Diffusion (CIFAR-10)\n",
    "\n",
    "## Step 1. Encode images into latent space (VAE)\n",
    "\n",
    "* Input image: $x \\in \\mathbb{R}^{3 \\times 32 \\times 32}$\n",
    "* The VAE encoder learns a Gaussian posterior:\n",
    "\n",
    "  $$\n",
    "  q_\\phi(z|x) = \\mathcal{N}\\!\\big(z;\\,\\mu_\\phi(x),\\,\\sigma_\\phi^2(x)\\big), \\quad z \\in \\mathbb{R}^{4 \\times 8 \\times 8}.\n",
    "  $$\n",
    "  - **$q_\\phi(z|x)$**: Probability distribution of the latent variable $z$ given an input $x$.\n",
    "  - **$\\mu_\\phi(x)$**: Predicted mean vector of the latent distribution.\n",
    "  - **$\\sigma^2_\\phi(x)$**: Predicted variance (or uncertainty) for each latent dimension.\n",
    "* Reparameterization trick: Allows gradients to flow through sampling by injecting noise explicitly.\n",
    "\n",
    "  $$\n",
    "  z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,I).\n",
    "  $$\n",
    "\n",
    "### VAE loss (β-VAE ELBO)\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{VAE}} = \\|x - \\hat{x}\\|_1 + \\beta \\cdot D_{\\mathrm{KL}}(q_\\phi(z|x)\\,\\|\\,\\mathcal{N}(0,I))\n",
    "$$\n",
    "\n",
    "* **Reconstruction term**: $\\|x-\\hat{x}\\|_1$\n",
    "\n",
    "  * Measures pixel-wise error between input and reconstruction\n",
    "* **KL divergence**:\n",
    "\n",
    "  $$\n",
    "  D_{\\mathrm{KL}} = \\frac{1}{2}\\sum\\big(\\mu^2 + \\sigma^2 - \\log\\sigma^2 - 1\\big)\n",
    "  $$\n",
    "\n",
    "  * Forces latent distribution to stay close to Gaussian prior $N(0,I)$\n",
    "* $\\beta$: hyperparameter controlling tradeoff between reconstruction and regularization\n",
    "\n",
    "\n",
    "\n",
    "## Step 2. Define forward noising process (Diffusion forward)\n",
    "\n",
    "### Noising process\n",
    "\n",
    "$$\n",
    "z_t = \\sqrt{\\bar\\alpha_t}\\,z_0 + \\sqrt{1-\\bar\\alpha_t}\\,\\epsilon,\\quad \\epsilon \\sim \\mathcal{N}(0,I)\n",
    "$$\n",
    "\n",
    "* $z_0$: clean latent from VAE\n",
    "* $z_t$: noisy version after $t$ steps\n",
    "* $\\bar\\alpha_t$: cumulative product of noise schedule (see below)\n",
    "* $\\epsilon$: standard Gaussian noise\n",
    "\n",
    "\n",
    "### Noise schedule\n",
    "\n",
    "* Per-step variance parameter:\n",
    "\n",
    "  $$\n",
    "  \\beta_t \\in (0,1)\n",
    "  $$\n",
    "* Per-step keep-rate:\n",
    "\n",
    "  $$\n",
    "  \\alpha_t = 1 - \\beta_t\n",
    "  $$\n",
    "* Cumulative keep-rate:\n",
    "\n",
    "  $$\n",
    "  \\bar\\alpha_t = \\prod_{s=1}^t \\alpha_s\n",
    "  $$\n",
    "\n",
    "Intuition: larger $t$ ⇒ more noise ⇒ $\\bar\\alpha_t$ gets smaller.\n",
    "\n",
    "\n",
    "## Step 3. Train the denoiser UNet (Reverse process)\n",
    "\n",
    "### Goal\n",
    "\n",
    "Train UNet $\\epsilon_\\theta$ to predict the noise added at each step:\n",
    "\n",
    "$$\n",
    "\\epsilon_\\theta: (z_t, t, y) \\mapsto \\hat{\\epsilon}\n",
    "$$\n",
    "\n",
    "\n",
    "### Loss function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{DDPM}} = \\mathbb{E}_{x,\\epsilon,t}\\;\\|\\epsilon - \\epsilon_\\theta(z_t,t,\\tilde{y})\\|^2\n",
    "$$\n",
    "\n",
    "* $\\epsilon$: true noise used in forward process\n",
    "* $\\hat{\\epsilon}$: predicted noise\n",
    "* $y$: class label (0–9 for CIFAR-10)\n",
    "* $\\tilde{y}$: either real label or “null” (unconditional) for classifier-free guidance\n",
    "\n",
    "## Step 4. Sampling (reverse diffusion)\n",
    "\n",
    "We start from pure noise $z_T \\sim \\mathcal{N}(0,I)$ and iteratively denoise.\n",
    "\n",
    "### Predict clean latent\n",
    "\n",
    "$$\n",
    "\\hat{z}_0 = \\frac{z_t - \\sqrt{1-\\bar\\alpha_t}\\,\\hat{\\epsilon}}{\\sqrt{\\bar\\alpha_t}}\n",
    "$$\n",
    "\n",
    "* Uses denoised noise $\\hat{\\epsilon}$ to recover approximation of the original latent.\n",
    "\n",
    "\n",
    "\n",
    "### DDPM update (stochastic)\n",
    "\n",
    "$$\n",
    "z_{t-1} = \\sqrt{\\bar\\alpha_{t-1}} \\hat{z}_0 + \\sqrt{1-\\bar\\alpha_{t-1}-\\sigma_t^2}\\,\\hat{\\epsilon} + \\sigma_t \\xi,\n",
    "\\quad \\xi\\sim\\mathcal{N}(0,I)\n",
    "$$\n",
    "\n",
    "* $\\sigma_t^2 = \\beta_t \\cdot \\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t}$: posterior variance\n",
    "* Adds extra Gaussian noise each step (ancestral sampling).\n",
    "\n",
    "\n",
    "\n",
    "### DDIM update (deterministic or stochastic)\n",
    "\n",
    "$$\n",
    "z_{t-1} = \\sqrt{\\bar\\alpha_{t-1}} \\hat{z}_0 + \\sqrt{1-\\bar\\alpha_{t-1}}\\,\\hat{\\epsilon},\\quad (\\eta=0)\n",
    "$$\n",
    "\n",
    "* With $\\eta=0$: purely deterministic, fewer steps (fast).\n",
    "* With $\\eta>0$: reintroduces stochasticity for more diversity.\n",
    "\n",
    "\n",
    "\n",
    "## Step 5. Classifier-Free Guidance at sampling\n",
    "Idea: make diffusion samples more faithful to the condition while staying realistic. Uses the same model trained to handle both conditional and unconditional cases. Gives control over fidelity vs. diversity of generations.\n",
    "\n",
    "Run the network twice:\n",
    "\n",
    "* Unconditional: $\\hat{\\epsilon}_u = \\epsilon_\\theta(z_t, t, \\varnothing)$\n",
    "* Conditional: $\\hat{\\epsilon}_c = \\epsilon_\\theta(z_t, t, y)$\n",
    "\n",
    "Combine:\n",
    "\n",
    "$$\n",
    "\\hat{\\epsilon} = \\hat{\\epsilon}_u + s \\cdot (\\hat{\\epsilon}_c - \\hat{\\epsilon}_u),\n",
    "$$\n",
    "\n",
    "where $s$ = guidance scale (>= 1).\n",
    "\n",
    "## Step 6. Decode latent back to image\n",
    "\n",
    "- $z_0$: The final latent after the diffusion process finishes denoising. $[B,4,8,8]$\n",
    "- After reaching $z_0$, decode with VAE:\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\text{Dec}_\\theta(z_0).\n",
    "$$\n",
    "\n",
    "This produces a CIFAR-10 image in $[B,3,32,32]$.\n",
    "\n",
    "## References\n",
    "\n",
    "| Step | Concept                                   | Paper                                                            |\n",
    "| ---- | ----------------------------------------- | ---------------------------------------------------------------- |\n",
    "| 1    | VAE (posterior, reparameterization)       | [Kingma & Welling 2013](https://arxiv.org/abs/1312.6114)         |\n",
    "| 1    | β-VAE (disentanglement, KL weighting)     | [Higgins et al. 2017](https://openreview.net/forum?id=Sy2fzU9gl) |\n",
    "| 2    | Forward diffusion (noising process)       | [Ho et al. 2020](https://arxiv.org/abs/2006.11239)               |\n",
    "| 3    | Noise prediction training objective       | [Ho et al. 2020](https://arxiv.org/abs/2006.11239)               |\n",
    "| 4    | DDPM sampling (ancestral reverse process) | [Ho et al. 2020](https://arxiv.org/abs/2006.11239)               |\n",
    "| 4    | DDIM sampling (fast, deterministic)       | [Song et al. 2021](https://arxiv.org/abs/2010.02502)             |\n",
    "| 5    | Classifier-Free Guidance (CFG)            | [Ho & Salimans 2021](https://arxiv.org/abs/2207.12598)           |\n",
    "| 6    | Latent Diffusion (VAE + DDPM combo)       | [Rombach et al. 2022](https://arxiv.org/abs/2112.10752)          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca18bc-3b99-479c-b7e8-aaae3a17ccbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
